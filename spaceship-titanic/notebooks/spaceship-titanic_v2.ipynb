{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d523e817-f059-4c5a-8c10-8e5fb6c5b0c2",
   "metadata": {},
   "source": [
    "# Spaceship Titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c8ad31-eda1-425b-8466-7df74c13a28a",
   "metadata": {},
   "source": [
    "Train dataframe description:\n",
    "- **PassengerId** - A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.\n",
    "- **HomePlanet** - The planet the passenger departed from, typically their planet of permanent residence.\n",
    "- **CryoSleep** - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.\n",
    "- **Cabin** - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.\n",
    "- **Destination** - The planet the passenger will be debarking to.\n",
    "- **Age** - The age of the passenger.\n",
    "- **VIP** - Whether the passenger has paid for special VIP service during the voyage.\n",
    "- **RoomService, FoodCourt, ShoppingMall, Spa, VRDeck** - Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.\n",
    "- **Name** - The first and last names of the passenger.\n",
    "- **Transported** - Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3148e35-6c0d-4349-bcc3-082ce5299d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some modules that we are going to use.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, LearningCurveDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b4b9d3-9325-4c46-a14a-1cbdb7e93fb7",
   "metadata": {},
   "source": [
    "## Define Train and Test dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af1f6457-8b75-4a45-9962-a4c331c0a3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_df shape: (8693, 14)\n",
      "               Age   RoomService     FoodCourt  ShoppingMall           Spa  \\\n",
      "count  8514.000000   8512.000000   8510.000000   8485.000000   8510.000000   \n",
      "mean     28.827930    224.687617    458.077203    173.729169    311.138778   \n",
      "std      14.489021    666.717663   1611.489240    604.696458   1136.705535   \n",
      "min       0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%      19.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "50%      27.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "75%      38.000000     47.000000     76.000000     27.000000     59.000000   \n",
      "max      79.000000  14327.000000  29813.000000  23492.000000  22408.000000   \n",
      "\n",
      "             VRDeck  \n",
      "count   8505.000000  \n",
      "mean     304.854791  \n",
      "std     1145.717189  \n",
      "min        0.000000  \n",
      "25%        0.000000  \n",
      "50%        0.000000  \n",
      "75%       46.000000  \n",
      "max    24133.000000  \n",
      "\n",
      "\n",
      "First 10 samples:\n",
      "  PassengerId HomePlanet CryoSleep  Cabin    Destination   Age    VIP  \\\n",
      "0     0001_01     Europa     False  B/0/P    TRAPPIST-1e  39.0  False   \n",
      "1     0002_01      Earth     False  F/0/S    TRAPPIST-1e  24.0  False   \n",
      "2     0003_01     Europa     False  A/0/S    TRAPPIST-1e  58.0   True   \n",
      "3     0003_02     Europa     False  A/0/S    TRAPPIST-1e  33.0  False   \n",
      "4     0004_01      Earth     False  F/1/S    TRAPPIST-1e  16.0  False   \n",
      "5     0005_01      Earth     False  F/0/P  PSO J318.5-22  44.0  False   \n",
      "6     0006_01      Earth     False  F/2/S    TRAPPIST-1e  26.0  False   \n",
      "7     0006_02      Earth      True  G/0/S    TRAPPIST-1e  28.0  False   \n",
      "8     0007_01      Earth     False  F/3/S    TRAPPIST-1e  35.0  False   \n",
      "9     0008_01     Europa      True  B/1/P    55 Cancri e  14.0  False   \n",
      "\n",
      "   RoomService  FoodCourt  ShoppingMall     Spa  VRDeck                Name  \\\n",
      "0          0.0        0.0           0.0     0.0     0.0     Maham Ofracculy   \n",
      "1        109.0        9.0          25.0   549.0    44.0        Juanna Vines   \n",
      "2         43.0     3576.0           0.0  6715.0    49.0       Altark Susent   \n",
      "3          0.0     1283.0         371.0  3329.0   193.0        Solam Susent   \n",
      "4        303.0       70.0         151.0   565.0     2.0   Willy Santantines   \n",
      "5          0.0      483.0           0.0   291.0     0.0   Sandie Hinetthews   \n",
      "6         42.0     1539.0           3.0     0.0     0.0  Billex Jacostaffey   \n",
      "7          0.0        0.0           0.0     0.0     NaN  Candra Jacostaffey   \n",
      "8          0.0      785.0          17.0   216.0     0.0       Andona Beston   \n",
      "9          0.0        0.0           0.0     0.0     0.0      Erraiam Flatic   \n",
      "\n",
      "   Transported  \n",
      "0        False  \n",
      "1         True  \n",
      "2        False  \n",
      "3        False  \n",
      "4         True  \n",
      "5         True  \n",
      "6         True  \n",
      "7         True  \n",
      "8         True  \n",
      "9         True  \n"
     ]
    }
   ],
   "source": [
    "# Define train and test dataframe.\n",
    "train_df = pd.read_csv(\"../inputdata/train.csv\", sep=\",\")\n",
    "test_df = pd.read_csv(\"../inputdata/test.csv\", sep=\",\")\n",
    "\n",
    "print(\"Train_df shape:\", train_df.shape)\n",
    "print(train_df.describe())\n",
    "\n",
    "print(\"\\n\\nFirst 10 samples:\")\n",
    "print(train_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ec9cde-911e-4fd3-a885-4402cebdab8e",
   "metadata": {},
   "source": [
    "## Explore the dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af95e8b-a875-45d9-8f14-2378a6cb022e",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1abca271-9a9a-4f55-bd88-dcaaa1e5593f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train missing values:\n",
      "PassengerId       0\n",
      "HomePlanet      201\n",
      "CryoSleep       217\n",
      "Cabin           199\n",
      "Destination     182\n",
      "Age             179\n",
      "VIP             203\n",
      "RoomService     181\n",
      "FoodCourt       183\n",
      "ShoppingMall    208\n",
      "Spa             183\n",
      "VRDeck          188\n",
      "Name            200\n",
      "Transported       0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Test missing values:\n",
      "PassengerId       0\n",
      "HomePlanet       87\n",
      "CryoSleep        93\n",
      "Cabin           100\n",
      "Destination      92\n",
      "Age              91\n",
      "VIP              93\n",
      "RoomService      82\n",
      "FoodCourt       106\n",
      "ShoppingMall     98\n",
      "Spa             101\n",
      "VRDeck           80\n",
      "Name             94\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train missing values:\")\n",
    "print(train_df.isna().sum())\n",
    "\n",
    "print(\"\\n\\nTest missing values:\")\n",
    "print(test_df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4914a00-b3ae-4502-b0d9-d96c102ee93d",
   "metadata": {},
   "source": [
    "### New feature: TotBill = RoomService + FoodCourt + ShoppingMall + Spa + VRDeck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d75d154-b547-455b-926e-93149d030ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column that contains the sum of the bills.\n",
    "train_df[\"TotBill\"] = train_df[\"RoomService\"] + train_df[\"FoodCourt\"] + train_df[\"ShoppingMall\"] + train_df[\"Spa\"] + train_df[\"VRDeck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "457e9770-f65c-47e1-9f3d-fb27887fcbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how the distribution changes if we fill with 0's the missing data.\n",
    "zero_fillna = (\n",
    "    train_df[\"RoomService\"].fillna(0) + \n",
    "    train_df[\"FoodCourt\"].fillna(0) + \n",
    "    train_df[\"ShoppingMall\"].fillna(0) + \n",
    "    train_df[\"Spa\"].fillna(0) + \n",
    "    train_df[\"VRDeck\"].fillna(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e22749e6-372c-4238-a137-a16619429270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's replace the TotBill column with the zero_fillna.\n",
    "\n",
    "# Train dataframe.\n",
    "train_df[\"TotBill\"] = zero_fillna\n",
    "\n",
    "# Test dataframe.\n",
    "test_df[\"TotBill\"] = (\n",
    "    test_df[\"RoomService\"].fillna(0) + \n",
    "    test_df[\"FoodCourt\"].fillna(0) + \n",
    "    test_df[\"ShoppingMall\"].fillna(0) + \n",
    "    test_df[\"Spa\"].fillna(0) + \n",
    "    test_df[\"VRDeck\"].fillna(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfe433e-b52b-4b4b-9685-35ee6e0c03ab",
   "metadata": {},
   "source": [
    "### RoomService, FoodCourt, ShoppingMall, Spa, VRDeck: manage missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70b77ce1-4b0d-4cdc-b1ad-44a967ae9793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataframe.\n",
    "for name in [\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]:\n",
    "    train_df[name] = train_df[name].fillna(0)\n",
    "\n",
    "# Test dataframe.\n",
    "for name in [\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]:\n",
    "    test_df[name] = test_df[name].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c249a2-6034-4323-a8bb-03c8df0914ff",
   "metadata": {},
   "source": [
    "### Categorize TotBill feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18e374f2-e630-4e9e-94f7-371106d8ee0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TotBill\n",
      "Low          3758\n",
      "Mid          2819\n",
      "High         1479\n",
      "Very high     637\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def categ_totbill(totbill):\n",
    "    if totbill <= 500:\n",
    "        return \"Low\"\n",
    "    elif totbill <= 1500:\n",
    "        return \"Mid\"\n",
    "    elif totbill <= 5000:\n",
    "        return \"High\"\n",
    "    else:\n",
    "        return \"Very high\"\n",
    "\n",
    "# Train dataframe.\n",
    "status_col = train_df[\"TotBill\"].apply(categ_totbill)\n",
    "print(status_col.value_counts())\n",
    "train_df[\"BillCateg\"] = status_col\n",
    "\n",
    "# Test dataframe.\n",
    "test_df[\"BillCateg\"] = test_df[\"TotBill\"].apply(categ_totbill)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f9760f-74d3-4196-88ff-c9462ec15ee2",
   "metadata": {},
   "source": [
    "### New feature: Group\n",
    "\n",
    "Extract the Group from the PassengerId column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "218f4e36-bf50-4c1a-986c-3b8e5fb696fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId\n",
      "4256    8\n",
      "5133    8\n",
      "8956    8\n",
      "8168    8\n",
      "0984    8\n",
      "       ..\n",
      "3469    1\n",
      "3468    1\n",
      "3467    1\n",
      "3465    1\n",
      "3491    1\n",
      "Name: count, Length: 6217, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def get_group(Id):\n",
    "    Id = Id.split(\"_\")\n",
    "    return Id[0]\n",
    "\n",
    "# Train dataframe.\n",
    "group_col = train_df[\"PassengerId\"].apply(get_group)\n",
    "print(group_col.value_counts())\n",
    "train_df[\"Group\"] = group_col\n",
    "\n",
    "# Train dataframe.\n",
    "test_df[\"Group\"] = test_df[\"PassengerId\"].apply(get_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d23296-deab-4e42-b32f-51814b02c775",
   "metadata": {},
   "source": [
    "### New feature: Deck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ca412f0-808a-459c-a3d5-d45f12fbdbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cabin\n",
      "F    2794\n",
      "G    2559\n",
      "E     876\n",
      "B     779\n",
      "C     747\n",
      "D     478\n",
      "A     256\n",
      "T       5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def get_deck(cabin):\n",
    "    if cabin is np.nan:\n",
    "        return np.nan\n",
    "    return cabin.split(\"/\")[0]\n",
    "\n",
    "# Train dataset.\n",
    "deck_col = train_df[\"Cabin\"].apply(get_deck)\n",
    "print(deck_col.value_counts())\n",
    "train_df[\"Deck\"] = deck_col\n",
    "\n",
    "# Test dataset.\n",
    "test_df[\"Deck\"] = test_df[\"Cabin\"].apply(get_deck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37151eb8-be87-4ce2-a4d6-f7da72e028d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataframe.\n",
    "train_df.loc[train_df[\"Deck\"] == \"T\", \"Deck\"] = \"F\"\n",
    "# Test dataframe.\n",
    "test_df.loc[test_df[\"Deck\"] == \"T\", \"Deck\"] = \"F\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9b93c98-1754-40bf-8aa0-0de3d7cb1752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tot passenger with nan Deck: 199\n",
      "Tot passenger with nan Deck that are part of a group of size > 1: 100\n",
      "\n",
      "Tot passenger travelling in a group larger than 1: 1412\n",
      "Tot passenger travelling in a group larger that 1 that have same Deck: 1107\n"
     ]
    }
   ],
   "source": [
    "print(\"Tot passenger with nan Deck:\", train_df[\"Deck\"].isna().sum())\n",
    "print(\"Tot passenger with nan Deck that are part of a group of size > 1:\", train_df.groupby([\"Group\"]).filter(lambda x: len(x) > 1)[\"Deck\"].isna().sum())\n",
    "print()\n",
    "print(\"Tot passenger travelling in a group larger than 1:\", len(train_df.groupby([\"Group\"]).filter(lambda x: len(x) > 1).groupby(\"Group\")))\n",
    "print(\"Tot passenger travelling in a group larger that 1 that have same Deck:\", len(train_df.groupby([\"Group\", \"Deck\"]).filter(lambda x: len(x) > 1).groupby(\"Group\").size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e393de7a-93bd-4435-be4c-d5b80576ab29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10793/3030994098.py:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train_df = train_df.groupby(\"Group\", group_keys=False).apply(fill_deck)\n",
      "/tmp/ipykernel_10793/3030994098.py:14: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test_df = test_df.groupby(\"Group\", group_keys=False).apply(fill_deck)\n"
     ]
    }
   ],
   "source": [
    "group_sizes = train_df[\"Group\"].map(train_df[\"Group\"].value_counts())\n",
    "\n",
    "def fill_deck(group):\n",
    "    if group[\"Deck\"].isnull().any():\n",
    "        non_null = group[\"Deck\"].dropna()\n",
    "        if not non_null.empty:\n",
    "            group[\"Deck\"] = group[\"Deck\"].fillna(non_null.iloc[0])\n",
    "    return group\n",
    "\n",
    "# Train dataframe.\n",
    "train_df = train_df.groupby(\"Group\", group_keys=False).apply(fill_deck)\n",
    "\n",
    "# Test dataframe.\n",
    "test_df = test_df.groupby(\"Group\", group_keys=False).apply(fill_deck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "853e8e84-6bc3-4903-9543-b1290f5279f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_deck_2(row):\n",
    "    if row[\"Deck\"] is not np.nan:\n",
    "        return row\n",
    "    elif row[\"HomePlanet\"] == \"Europa\":\n",
    "        row[\"Deck\"] = np.random.choice([\"B\", \"A\", \"C\"])\n",
    "        return row\n",
    "    elif row[\"HomePlanet\"] == \"Earth\":\n",
    "        row[\"Deck\"] = np.random.choice([\"F\", \"G\", \"E\"])\n",
    "        return row\n",
    "    elif row[\"HomePlanet\"] == \"Mars\":\n",
    "        row[\"Deck\"] = np.random.choice([\"F\", \"D\", \"E\"])\n",
    "        return row\n",
    "    else:\n",
    "        row[\"Deck\"] = np.random.choice([\"F\", \"G\"])\n",
    "        return row\n",
    "\n",
    "# Train dataframe.\n",
    "train_df = train_df.apply(fill_deck_2, axis=1)\n",
    "\n",
    "# Test dataframe (use the same probability used before)\n",
    "test_df = test_df.apply(fill_deck_2, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205fb392-925b-4f5a-a7dc-583a4f969a37",
   "metadata": {},
   "source": [
    "### New feature: CabinNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fc9306a-44da-48ca-a423-5d435d6f3187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cabin\n",
      "-1       199\n",
      " 82       28\n",
      " 19       22\n",
      " 86       22\n",
      " 176      21\n",
      "        ... \n",
      " 1863      1\n",
      " 1864      1\n",
      " 1865      1\n",
      " 1761      1\n",
      " 1866      1\n",
      "Name: count, Length: 1818, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def get_cabinnum(cabin):\n",
    "    if cabin == \"FakeCabin\":\n",
    "        return -1\n",
    "    return int(cabin.split(\"/\")[1])\n",
    "\n",
    "# Train dataframe.\n",
    "cabinnum_col = train_df[\"Cabin\"].fillna(\"FakeCabin\").apply(get_cabinnum)\n",
    "print(cabinnum_col.value_counts())\n",
    "train_df[\"CabinNum\"] = cabinnum_col\n",
    "\n",
    "# Test dataframe.\n",
    "test_df[\"CabinNum\"] = test_df[\"Cabin\"].fillna(\"FakeCabin\").apply(get_cabinnum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253f400e-ab45-473a-9b7e-06a6d9e2e687",
   "metadata": {},
   "source": [
    "### New feature: CabinSide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e6e0338-857f-4e25-867a-6b4ffb640968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cabin\n",
      "S    4394\n",
      "P    4299\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def get_cabinside(cabin):\n",
    "    if cabin == \"FakeCabin\":\n",
    "        return np.random.choice([\"S\", \"P\"])\n",
    "    return cabin.split(\"/\")[2]\n",
    "\n",
    "# Train dataframe.\n",
    "cabinside_col = train_df[\"Cabin\"].fillna(\"FakeCabin\").apply(get_cabinside)\n",
    "print(cabinside_col.value_counts())\n",
    "train_df[\"CabinSide\"] = cabinside_col\n",
    "\n",
    "# Test dataframe.\n",
    "test_df[\"CabinSide\"] = test_df[\"Cabin\"].fillna(\"FakeCabin\").apply(get_cabinside)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3533d8f-ad97-4552-974d-28256282621b",
   "metadata": {},
   "source": [
    "### HomePlanet, manage missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa3ab0b3-76cb-4ca1-8a4e-c452781e2886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tot passenger with nan HomePlanet that are part of a group of size > 1: 92\n",
      "Tot passenger with nan HomePlanet that are lonely in the group: 109\n",
      "\n",
      "Tot passenger travelling in a group larger than 1: 1412\n",
      "Tot passenger travelling in a group larger that 1 that have same HomePlanet: 1370\n"
     ]
    }
   ],
   "source": [
    "print(\"Tot passenger with nan HomePlanet that are part of a group of size > 1:\", train_df.groupby([\"Group\"]).filter(lambda x: len(x) > 1)[\"HomePlanet\"].isna().sum())\n",
    "print(\"Tot passenger with nan HomePlanet that are lonely in the group:\", train_df.groupby([\"Group\"]).filter(lambda x: len(x) == 1)[\"HomePlanet\"].isna().sum())\n",
    "print()\n",
    "print(\"Tot passenger travelling in a group larger than 1:\", len(train_df.groupby([\"Group\"]).filter(lambda x: len(x) > 1).groupby(\"Group\")))\n",
    "print(\"Tot passenger travelling in a group larger that 1 that have same HomePlanet:\", len(train_df.groupby([\"Group\", \"HomePlanet\"]).filter(lambda x: len(x) > 1).groupby(\"Group\").size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3414b902-f391-4dc5-9c23-d3c59c590edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10793/2507866815.py:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train_df = train_df.groupby(\"Group\", group_keys=False).apply(fill_home_planet)\n",
      "/tmp/ipykernel_10793/2507866815.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test_df = test_df.groupby(\"Group\", group_keys=False).apply(fill_home_planet)\n"
     ]
    }
   ],
   "source": [
    "# We can try the following: consider the passenger that have a missing HomePlanet. If they are travelling in a group larger than one,\n",
    "# then set missing HomePlanet to a value taken from one of the other member of the group.\n",
    "\n",
    "group_sizes = train_df[\"Group\"].map(train_df[\"Group\"].value_counts())\n",
    "\n",
    "def fill_home_planet(group):\n",
    "    if group[\"HomePlanet\"].isnull().any():\n",
    "        non_null_planets = group[\"HomePlanet\"].dropna()\n",
    "        if not non_null_planets.empty:\n",
    "            group[\"HomePlanet\"] = group[\"HomePlanet\"].fillna(non_null_planets.iloc[0])\n",
    "    return group\n",
    "\n",
    "# Train dataframe.\n",
    "old_homeplanet_col = train_df[\"HomePlanet\"].dropna()\n",
    "train_df = train_df.groupby(\"Group\", group_keys=False).apply(fill_home_planet)\n",
    "\n",
    "# Test dataframe.\n",
    "test_df = test_df.groupby(\"Group\", group_keys=False).apply(fill_home_planet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6530dbd-da08-49ab-a36d-c9fdf1b137ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are more missing values. Let's use the relation Deck-HomePlanet.\n",
    "np.random.seed(12)\n",
    "\n",
    "def fill_home_planet_2(row):\n",
    "    if row[\"HomePlanet\"] is not np.nan:\n",
    "        return row\n",
    "    elif row[\"Deck\"] in [\"A\", \"B\", \"C\"]:\n",
    "        row[\"HomePlanet\"] = \"Europa\"\n",
    "        return row\n",
    "    elif row[\"Deck\"] == \"G\":\n",
    "        row[\"HomePlanet\"] = \"Earth\"\n",
    "        return row\n",
    "    elif row[\"Deck\"] == \"F\":\n",
    "        row[\"HomePlanet\"] = np.random.choice([\"Earth\", \"Mars\"])\n",
    "        return row\n",
    "    elif row[\"Deck\"] == \"D\":\n",
    "        row[\"HomePlanet\"] = np.random.choice([\"Europa\", \"Mars\"])\n",
    "        return row\n",
    "    elif row[\"Deck\"] == \"E\":\n",
    "        row[\"HomePlanet\"] = np.random.choice([\"Europa\", \"Mars\", \"Earth\"])\n",
    "        return row\n",
    "    elif row[\"Deck\"] == \"FakeDeck\":\n",
    "        row[\"HomePlanet\"] = np.random.choice([\"Europa\", \"Mars\", \"Earth\"])\n",
    "        return row\n",
    "    else:\n",
    "        row[\"HomePlanet\"] = \"Earth\"\n",
    "        return row\n",
    "\n",
    "# Train dataframe.\n",
    "train_df = train_df.apply(fill_home_planet_2, axis=1)\n",
    "\n",
    "# Test dataframe (use the same probability used before)\n",
    "test_df = test_df.apply(fill_home_planet_2, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67bfb5c-55b8-41d1-9c85-01e491eae638",
   "metadata": {},
   "source": [
    "### Destination, manage missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfd6ed36-caef-4ea2-8ec5-81c9f6eced12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataframe.\n",
    "train_df[\"Destination\"] = train_df[\"Destination\"].fillna(\"TRAPPIST-1e\")\n",
    "# Test dataframe.\n",
    "test_df[\"Destination\"] = test_df[\"Destination\"].fillna(\"TRAPPIST-1e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42225d3-27b5-4240-b02d-88b0c7bd1bfc",
   "metadata": {},
   "source": [
    "### VIP, manage missing values\n",
    "\n",
    "Just fill the missing values with False since there are very few VIPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7284ac12-477f-42db-ada9-b6fd17adaf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataframe.\n",
    "train_df[\"VIP\"] = train_df[\"VIP\"].fillna(0)\n",
    "# Test dataframe.\n",
    "test_df[\"VIP\"] = test_df[\"VIP\"].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3705049-9e26-4616-83d0-4252a0f03d41",
   "metadata": {},
   "source": [
    "### Age, manage missing values\n",
    "Up to this point there are two columns that have missing values: Age and CryoSleep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbabd0a2-d24e-4205-b904-8cbce7adedba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with the median.\n",
    "median_age_col = train_df['Age'].fillna(train_df['Age'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6afb1575-9df5-45c5-ac21-85e09f152138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataframe.\n",
    "train_df[\"Age\"] = median_age_col\n",
    "# Test dataframe.\n",
    "test_df[\"Age\"] = test_df[\"Age\"].fillna(test_df[\"Age\"].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d0e7e0-a199-4274-97b9-415fe5fe8238",
   "metadata": {},
   "source": [
    "### Encode Age feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "460c45c0-bb1c-4170-95b9-495c08c66485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age\n",
      "Adult          2981\n",
      "Young adult    2847\n",
      "Teen           1629\n",
      "Child           806\n",
      "Elderly         430\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def categ_age(age):\n",
    "    if age <= 12:\n",
    "        return \"Child\"\n",
    "    elif age <= 20:\n",
    "        return \"Teen\"\n",
    "    elif age <= 30:\n",
    "        return \"Young adult\"\n",
    "    elif age <= 55:\n",
    "        return \"Adult\"\n",
    "    else:\n",
    "        return \"Elderly\"\n",
    "\n",
    "# Train dataframe.\n",
    "new_age_col = train_df[\"Age\"].apply(categ_age)\n",
    "print(new_age_col.value_counts())\n",
    "train_df[\"Age\"] = new_age_col\n",
    "\n",
    "# Test dataframe.\n",
    "test_df[\"Age\"] = test_df[\"Age\"].apply(categ_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca6a21a-5af3-46db-8a23-831304dbc61b",
   "metadata": {},
   "source": [
    "### CryoSleep, manage missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda3005c-0715-4670-8dee-cd78c0eb82dd",
   "metadata": {},
   "source": [
    "Since passengers in CryoSleep are confinated in their cabin they must have TotBill = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50407fe1-3def-4c51-8de6-e16727b11525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tot passenger in CryoSleep: 3037\n",
      "Tot passenger in CryoSleep with TotBill = 0: 3037\n",
      "Tot passenger in with TotBill = 0: 3653\n"
     ]
    }
   ],
   "source": [
    "# How many passenger in CryoSleep have a TotBill equal to zero?\n",
    "print(\"Tot passenger in CryoSleep:\", len(train_df[train_df[\"CryoSleep\"] == 1]))\n",
    "print(\"Tot passenger in CryoSleep with TotBill = 0:\", len(train_df[(train_df[\"TotBill\"] == 0) & (train_df[\"CryoSleep\"] == 1)]))\n",
    "print(\"Tot passenger in with TotBill = 0:\", len(train_df[train_df[\"TotBill\"] == 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70c1bb6-75e1-4c47-9b67-c300c6f96d14",
   "metadata": {},
   "source": [
    "$3037 / 3653 \\approx 83\\%$ of the passenger with TotBill = 0 are in CryoSleep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "635e54a3-05f2-4ea6-bab0-e54b60949dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataframe.\n",
    "train_df.loc[train_df[\"CryoSleep\"].isna(), \"CryoSleep\"] = (train_df[\"TotBill\"] == 0).astype(int)\n",
    "\n",
    "# Test dataframe.\n",
    "test_df.loc[test_df[\"CryoSleep\"].isna(), \"CryoSleep\"] = (test_df[\"TotBill\"] == 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa5f4ef-7fc2-44e7-933a-7d96243e5f2f",
   "metadata": {},
   "source": [
    "### Drop some features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5bf8c396-1062-4143-ae3c-167ae61c174c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HomePlanet      0\n",
      "CryoSleep       0\n",
      "Destination     0\n",
      "Age             0\n",
      "VIP             0\n",
      "RoomService     0\n",
      "FoodCourt       0\n",
      "ShoppingMall    0\n",
      "Spa             0\n",
      "VRDeck          0\n",
      "Transported     0\n",
      "BillCateg       0\n",
      "Deck            0\n",
      "CabinSide       0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "\n",
      "PassengerId     0\n",
      "HomePlanet      0\n",
      "CryoSleep       0\n",
      "Destination     0\n",
      "Age             0\n",
      "VIP             0\n",
      "RoomService     0\n",
      "FoodCourt       0\n",
      "ShoppingMall    0\n",
      "Spa             0\n",
      "VRDeck          0\n",
      "BillCateg       0\n",
      "Deck            0\n",
      "CabinSide       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.drop(columns=[\"PassengerId\", \"Cabin\", \"Name\", \"TotBill\", \"Group\", \"CabinNum\"], axis=1)\n",
    "print(train_df.isna().sum())\n",
    "print(\"\\n\\n\")\n",
    "test_df = test_df.drop(columns=[\"Cabin\", \"Name\", \"TotBill\", \"Group\", \"CabinNum\"], axis=1)\n",
    "print(test_df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb10aeb7-e8fb-4417-a047-46f7fed15d9d",
   "metadata": {},
   "source": [
    "## Encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88ee8316-ef7d-4321-a4fc-207e5d00273a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tot number of samples in train set: 8693\n",
      "Tot number of samples in test set: 4277\n",
      "Features names: ['HomePlanet', 'CryoSleep', 'Destination', 'Age', 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'BillCateg', 'Deck', 'CabinSide']\n"
     ]
    }
   ],
   "source": [
    "X_train = train_df.dropna().drop(columns=\"Transported\")\n",
    "X_test = test_df.dropna()\n",
    "Y_train = train_df.dropna()[\"Transported\"]\n",
    "\n",
    "print(\"Tot number of samples in train set:\", X_train.shape[0])\n",
    "print(\"Tot number of samples in test set:\", X_test.shape[0])\n",
    "features_names = list(X_train.columns)\n",
    "print(\"Features names:\", features_names)\n",
    "\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "Y_train = Y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ad78876-55f3-4179-9f51-e5edfd3f9c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column HomePlanet (index: 0) -- data type: <class 'str'>\n",
      "Column CryoSleep (index: 1) -- data type: <class 'bool'>\n",
      "Column Destination (index: 2) -- data type: <class 'str'>\n",
      "Column Age (index: 3) -- data type: <class 'str'>\n",
      "Column VIP (index: 4) -- data type: <class 'bool'>\n",
      "Column RoomService (index: 5) -- data type: <class 'float'>\n",
      "Column FoodCourt (index: 6) -- data type: <class 'float'>\n",
      "Column ShoppingMall (index: 7) -- data type: <class 'float'>\n",
      "Column Spa (index: 8) -- data type: <class 'float'>\n",
      "Column VRDeck (index: 9) -- data type: <class 'float'>\n",
      "Column BillCateg (index: 10) -- data type: <class 'str'>\n",
      "Column Deck (index: 11) -- data type: <class 'str'>\n",
      "Column CabinSide (index: 12) -- data type: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Print the data type of each column.\n",
    "for index_col, name_col in zip(range(X_train.shape[1]), features_names):\n",
    "    print(f\"Column {name_col} (index: {index_col}) -- data type: {type(X_train[0, index_col])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23e9cfdc-a106-4c13-bd0e-591b68309bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column HomePlanet (index: 0) -- data type: <class 'numpy.int64'>\n",
      "Column CryoSleep (index: 1) -- data type: <class 'numpy.int64'>\n",
      "Column Destination (index: 2) -- data type: <class 'numpy.int64'>\n",
      "Column Age (index: 3) -- data type: <class 'numpy.int64'>\n",
      "Column VIP (index: 4) -- data type: <class 'numpy.int64'>\n",
      "Column RoomService (index: 5) -- data type: <class 'numpy.int64'>\n",
      "Column FoodCourt (index: 6) -- data type: <class 'numpy.int64'>\n",
      "Column ShoppingMall (index: 7) -- data type: <class 'numpy.int64'>\n",
      "Column Spa (index: 8) -- data type: <class 'numpy.int64'>\n",
      "Column VRDeck (index: 9) -- data type: <class 'numpy.int64'>\n",
      "Column BillCateg (index: 10) -- data type: <class 'numpy.int64'>\n",
      "Column Deck (index: 11) -- data type: <class 'numpy.int64'>\n",
      "Column CabinSide (index: 12) -- data type: <class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "# Encode the categorical variables.\n",
    "to_encode = np.array([0, 2, 3, 10, 11, 12])\n",
    "\n",
    "enc = OrdinalEncoder(dtype=int)\n",
    "\n",
    "enc.fit(X_train[:, to_encode])\n",
    "X_train[:, to_encode] = enc.transform(X_train[:, to_encode])\n",
    "X_test[:, to_encode+1] = enc.transform(X_test[:, to_encode+1])\n",
    "\n",
    "X_train = X_train.astype(int)\n",
    "X_test = X_test.astype(int)\n",
    "\n",
    "# Print the data type of each column.\n",
    "for index_col, name_col in zip(range(X_train.shape[1]), features_names):\n",
    "    print(f\"Column {name_col} (index: {index_col}) -- data type: {type(X_train[0, index_col])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01e1f5c4-711e-4728-9246-7dde75a2e2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of data for training and deciding parameters: 6954\n",
      "Amount of data for test: 1739\n"
     ]
    }
   ],
   "source": [
    "m = X_train.shape[0]\n",
    "m_train = int(4./5. * m)\n",
    "m_val = m - m_train\n",
    "\n",
    "print(\"Amount of data for training and deciding parameters:\", m_train)\n",
    "print(\"Amount of data for test:\", m_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b837fcd0-75d8-47b3-af78-82524f85bcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=m_val/m, random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c91449c-04f1-4ae2-bb6d-f8681fd657af",
   "metadata": {},
   "source": [
    "## Standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2a55a34-1421-4d95-8945-420e8ac22a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standardize age and fare columns.\n",
    "#to_scale = np.array([5, 6, 7, 8, 9])\n",
    "\n",
    "# Create a copy for each set.\n",
    "X_train_scaled = np.copy(X_train)\n",
    "X_val_scaled = np.copy(X_val)\n",
    "X_test_scaled = np.copy(X_test)\n",
    "\n",
    "# Scale data.\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled[:, 1:] = scaler.transform(X_test[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c782ad-39e0-4e35-9a05-07de76891820",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "76849af2-d556-46aa-b530-a7c3fbf3790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        super(NN, self).__init__()\n",
    "        torch.manual_seed(12)\n",
    "\n",
    "        # Define the structure.\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(1, len(layers)):\n",
    "            self.layers.append(nn.Linear(layers[i-1], layers[i]))\n",
    "        self.layers.append(nn.Linear(layers[-1], 1)) # Last layer\n",
    "        \n",
    "        # Acivaction function.\n",
    "        self.actfunc = nn.ReLU() \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            out = layer(out)\n",
    "            if i < len(self.layers) - 1:\n",
    "                out = self.actfunc(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65a06fc1-c3c5-428f-974e-e346b013c4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X).squeeze(1)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f8266626-313f-425f-91c9-018b61ea22f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X).squeeze(1)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            pred = (pred >= 0.5).float()\n",
    "            correct += (pred == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bcf8a176-1f9d-4783-bbea-b9e1e0a5a4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NN(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=13, out_features=5, bias=True)\n",
      "    (1): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (2): Linear(in_features=5, out_features=1, bias=True)\n",
      "  )\n",
      "  (actfunc): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Set device.\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define the Neural Network\n",
    "# 2 hidden layers with 5 notes each.\n",
    "input_size = len(features_names)\n",
    "layers = {\"layers\": [input_size, 5, 5]}\n",
    "model = NN(**layers).to(device)\n",
    "print(model)\n",
    "\n",
    "# Define the loss function.\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# Define the optimizer.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train dataset and dataloader.\n",
    "train_ds = torch.utils.data.TensorDataset(torch.Tensor(X_train_scaled), torch.Tensor(Y_train))\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "# Val dataset and dataloader.\n",
    "val_ds = torch.utils.data.TensorDataset(torch.Tensor(X_val_scaled), torch.Tensor(Y_val))\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_ds, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f0bbd01-6e14-4396-a7a6-1df2f06acc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.699854  [   64/ 6954]\n",
      "loss: 0.677670  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 52.0%, Avg loss: 0.687897 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.687113  [   64/ 6954]\n",
      "loss: 0.595921  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.601150 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.622571  [   64/ 6954]\n",
      "loss: 0.533244  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.515638 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.509400  [   64/ 6954]\n",
      "loss: 0.415478  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.481796 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.481192  [   64/ 6954]\n",
      "loss: 0.330118  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.465062 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.467720  [   64/ 6954]\n",
      "loss: 0.460935  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.465429 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.523608  [   64/ 6954]\n",
      "loss: 0.380048  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.463281 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.385951  [   64/ 6954]\n",
      "loss: 0.531284  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.469013 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.458897  [   64/ 6954]\n",
      "loss: 0.445940  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.462792 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.486580  [   64/ 6954]\n",
      "loss: 0.262170  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.464300 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.326225  [   64/ 6954]\n",
      "loss: 0.491161  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.465539 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.519234  [   64/ 6954]\n",
      "loss: 0.433946  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.466887 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.463063  [   64/ 6954]\n",
      "loss: 0.460860  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.456867 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.466778  [   64/ 6954]\n",
      "loss: 0.530003  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.464340 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.542390  [   64/ 6954]\n",
      "loss: 0.445090  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.458932 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.391489  [   64/ 6954]\n",
      "loss: 0.383617  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.454112 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.447607  [   64/ 6954]\n",
      "loss: 0.396510  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.449116 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.430174  [   64/ 6954]\n",
      "loss: 0.367018  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.453938 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.384687  [   64/ 6954]\n",
      "loss: 0.431152  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.461866 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.416907  [   64/ 6954]\n",
      "loss: 0.514706  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.459720 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.533643  [   64/ 6954]\n",
      "loss: 0.500706  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.457434 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.404223  [   64/ 6954]\n",
      "loss: 0.373014  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.454215 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.340689  [   64/ 6954]\n",
      "loss: 0.411797  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.451476 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.393639  [   64/ 6954]\n",
      "loss: 0.362978  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.450035 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.474854  [   64/ 6954]\n",
      "loss: 0.430688  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 0.455477 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.489818  [   64/ 6954]\n",
      "loss: 0.505405  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.450424 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.360082  [   64/ 6954]\n",
      "loss: 0.302066  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.456256 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.597220  [   64/ 6954]\n",
      "loss: 0.273633  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.456486 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.363227  [   64/ 6954]\n",
      "loss: 0.408004  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.453148 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.337842  [   64/ 6954]\n",
      "loss: 0.410121  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.452349 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.463790  [   64/ 6954]\n",
      "loss: 0.341018  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 0.450628 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.415395  [   64/ 6954]\n",
      "loss: 0.447977  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.451917 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.326447  [   64/ 6954]\n",
      "loss: 0.450391  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.451270 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.433543  [   64/ 6954]\n",
      "loss: 0.568069  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.447998 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.416541  [   64/ 6954]\n",
      "loss: 0.478723  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 0.445506 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.456802  [   64/ 6954]\n",
      "loss: 0.475368  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.450591 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.414962  [   64/ 6954]\n",
      "loss: 0.454174  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.451746 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.407796  [   64/ 6954]\n",
      "loss: 0.417188  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.448834 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.504507  [   64/ 6954]\n",
      "loss: 0.531262  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.448714 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.415199  [   64/ 6954]\n",
      "loss: 0.565049  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.446220 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.525012  [   64/ 6954]\n",
      "loss: 0.399244  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.448787 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.492008  [   64/ 6954]\n",
      "loss: 0.547701  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.451242 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.498082  [   64/ 6954]\n",
      "loss: 0.400429  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.458800 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.403821  [   64/ 6954]\n",
      "loss: 0.390709  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.450498 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.393225  [   64/ 6954]\n",
      "loss: 0.556183  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.444604 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.370006  [   64/ 6954]\n",
      "loss: 0.440850  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.459336 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.333933  [   64/ 6954]\n",
      "loss: 0.447219  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.446759 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.368016  [   64/ 6954]\n",
      "loss: 0.347302  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.447626 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.365483  [   64/ 6954]\n",
      "loss: 0.352958  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.442928 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.410670  [   64/ 6954]\n",
      "loss: 0.415300  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.442374 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.422828  [   64/ 6954]\n",
      "loss: 0.340676  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.448877 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.576421  [   64/ 6954]\n",
      "loss: 0.386661  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.446146 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.491529  [   64/ 6954]\n",
      "loss: 0.422478  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.441970 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.356153  [   64/ 6954]\n",
      "loss: 0.432112  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.450978 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.350540  [   64/ 6954]\n",
      "loss: 0.404971  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.452397 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.406750  [   64/ 6954]\n",
      "loss: 0.364832  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.441921 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.434796  [   64/ 6954]\n",
      "loss: 0.286022  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.444429 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.353290  [   64/ 6954]\n",
      "loss: 0.535033  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.450723 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.501900  [   64/ 6954]\n",
      "loss: 0.443301  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.450703 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.352207  [   64/ 6954]\n",
      "loss: 0.475910  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.444552 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.408068  [   64/ 6954]\n",
      "loss: 0.396019  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.453303 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.363007  [   64/ 6954]\n",
      "loss: 0.503212  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.453478 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.447916  [   64/ 6954]\n",
      "loss: 0.463664  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.449544 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.330470  [   64/ 6954]\n",
      "loss: 0.450539  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.451655 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.410940  [   64/ 6954]\n",
      "loss: 0.438907  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.447307 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.484953  [   64/ 6954]\n",
      "loss: 0.460928  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.453050 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.419625  [   64/ 6954]\n",
      "loss: 0.549988  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.446302 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.412986  [   64/ 6954]\n",
      "loss: 0.611278  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.443866 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.342874  [   64/ 6954]\n",
      "loss: 0.436072  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.446075 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.407899  [   64/ 6954]\n",
      "loss: 0.306983  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.450882 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.461468  [   64/ 6954]\n",
      "loss: 0.403426  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.442323 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.354227  [   64/ 6954]\n",
      "loss: 0.325882  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.441651 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.490514  [   64/ 6954]\n",
      "loss: 0.555273  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.454932 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.347530  [   64/ 6954]\n",
      "loss: 0.460633  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.453065 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.327568  [   64/ 6954]\n",
      "loss: 0.428066  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.443992 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.468920  [   64/ 6954]\n",
      "loss: 0.420300  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.448588 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.410857  [   64/ 6954]\n",
      "loss: 0.445950  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.445336 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.330065  [   64/ 6954]\n",
      "loss: 0.426218  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.451456 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.410158  [   64/ 6954]\n",
      "loss: 0.443949  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.442487 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.403026  [   64/ 6954]\n",
      "loss: 0.431131  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 0.452183 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.443868  [   64/ 6954]\n",
      "loss: 0.408473  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.443828 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.417172  [   64/ 6954]\n",
      "loss: 0.484768  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.454089 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.357871  [   64/ 6954]\n",
      "loss: 0.550295  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.7%, Avg loss: 0.450746 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.308961  [   64/ 6954]\n",
      "loss: 0.496042  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.459639 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.419671  [   64/ 6954]\n",
      "loss: 0.403678  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.443340 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.337303  [   64/ 6954]\n",
      "loss: 0.490547  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.453092 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.452065  [   64/ 6954]\n",
      "loss: 0.466018  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.450635 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.461716  [   64/ 6954]\n",
      "loss: 0.306254  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.451006 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.377201  [   64/ 6954]\n",
      "loss: 0.386574  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.444446 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.478294  [   64/ 6954]\n",
      "loss: 0.487890  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.447297 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.352667  [   64/ 6954]\n",
      "loss: 0.292957  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.453459 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.410908  [   64/ 6954]\n",
      "loss: 0.448368  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.447225 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.437267  [   64/ 6954]\n",
      "loss: 0.358130  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.446709 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.347733  [   64/ 6954]\n",
      "loss: 0.526400  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.455641 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.451727  [   64/ 6954]\n",
      "loss: 0.506580  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.442598 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.314971  [   64/ 6954]\n",
      "loss: 0.395021  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.447616 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.347743  [   64/ 6954]\n",
      "loss: 0.463623  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 78.3%, Avg loss: 0.450434 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.380902  [   64/ 6954]\n",
      "loss: 0.349433  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.443064 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.351144  [   64/ 6954]\n",
      "loss: 0.362888  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.448729 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.370607  [   64/ 6954]\n",
      "loss: 0.576320  [ 6464/ 6954]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.449618 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, loss_fn, optimizer)\n",
    "    test(val_loader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fafc510-1b8d-48da-b513-1cf6116ec49b",
   "metadata": {},
   "source": [
    "## Create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d015fba-ed56-45ef-88be-183eab743a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your submission was successfully saved!\n"
     ]
    }
   ],
   "source": [
    "# Compute predictions.\n",
    "#predictions = rf_gs.best_estimator_.predict(X_test_scaled[:, 1:])\n",
    "model.eval()\n",
    "predictions = [0] * X_test_scaled.shape[0]\n",
    "X_test_tensor = torch.Tensor(X_test_scaled[:, 1:])\n",
    "for i, x in enumerate(X_test_tensor):\n",
    "    with torch.no_grad():\n",
    "        x = x.to(device)\n",
    "        pred = model(x)\n",
    "        pred = (pred >= 0.5).int()\n",
    "        predictions[i] = bool(pred.to(\"cpu\"))\n",
    "\n",
    "#print(predictions)\n",
    "\n",
    "# Save predictions.\n",
    "output = pd.DataFrame({'PassengerId': test_df[\"PassengerId\"], 'Transported': predictions}) \n",
    "output.to_csv('../outputdata/submission.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1b3ce3-6af1-49ab-8eeb-4b9a99c2a62f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
